#!/usr/bin/env python3
"""
C.U.T.E. Requirements Scorer for Agentic SDD

Scores requirements.md on 4 dimensions:
- C (Completeness): 0-25
- U (Unambiguity): 0-25
- T (Testability): 0-25
- E (EARS compliance): 0-25

Total: 0-100
"""

import argparse
import json
import re
from pathlib import Path

# Regex patterns
RE_REQ_HEADER = re.compile(r"^###\s+(REQ-\d{3})\s*:\s*(.+?)\s*$", re.MULTILINE)
RE_SECTION = re.compile(r"^##\s+(.+?)\s*$", re.MULTILINE)

# EARS patterns (JP + EN) â€” intentionally strict
EARS_PATTERNS = [
    # English
    re.compile(r"^The system shall .+"),
    re.compile(r"^When .+, the system shall .+"),
    re.compile(r"^While .+, the system shall .+"),
    re.compile(r"^If .+, then the system shall .+"),
    re.compile(r"^Where .+ is (enabled|included).+, the system shall .+"),
    # Japanese
    re.compile(r"^ã‚·ã‚¹ãƒ†ãƒ ã¯.+ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚?$"),
    re.compile(r"^.+ã¨ãã€ã‚·ã‚¹ãƒ†ãƒ ã¯.+ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚?$"),
    re.compile(r"^.+ã®é–“ã€ã‚·ã‚¹ãƒ†ãƒ ã¯.+ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚?$"),
    re.compile(r"^.+å ´åˆã€ã‚·ã‚¹ãƒ†ãƒ ã¯.+ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚?$"),
    re.compile(r"^.+ãŒæœ‰åŠ¹ãªå ´åˆã€ã‚·ã‚¹ãƒ†ãƒ ã¯.+ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚?$"),
]

RE_EARS_LINE = re.compile(r"^-?\s*è¦ä»¶æ–‡\(EARS\)\s*:\s*(.+)\s*$", re.MULTILINE)
RE_TYPE_LINE = re.compile(r"^-?\s*ç¨®åˆ¥\s*:\s*(.+)\s*$", re.MULTILINE)
RE_PRIORITY_LINE = re.compile(r"^-?\s*å„ªå…ˆåº¦\s*:\s*(.+)\s*$", re.MULTILINE)
RE_AT_LINE = re.compile(r"^\s*-\s*AT-\d+\s*:\s*(.+)\s*$", re.MULTILINE)
RE_EH_LINE = re.compile(r"^\s*-\s*EH-\d+\s*:\s*(.+)\s*$", re.MULTILINE)


def strip_code_fences(md: str) -> str:
    """Remove fenced code blocks to avoid false positives"""
    return re.sub(r"```.*?```", "", md, flags=re.DOTALL)


def load_banned_words(path: Path):
    """Load banned words list from file"""
    words = []
    if not path.exists():
        return words
    for line in path.read_text(encoding="utf-8").splitlines():
        w = line.strip()
        if not w or w.startswith("#"):
            continue
        words.append(w)
    return words


def count_banned(md: str, banned_words):
    """Count banned word occurrences"""
    text = strip_code_fences(md)
    total = 0
    hits = []
    for w in banned_words:
        c = len(re.findall(re.escape(w), text, flags=re.IGNORECASE))
        if c:
            total += c
            hits.append({"word": w, "count": c})
    return total, hits


def ears_ok(s: str) -> bool:
    """Check if string matches EARS pattern"""
    s = s.strip()
    return any(p.match(s) for p in EARS_PATTERNS)


def main():
    ap = argparse.ArgumentParser(description="C.U.T.E. Requirements Scorer")
    ap.add_argument("requirements_md", type=str, help="Path to requirements.md")
    ap.add_argument("--out-json", type=str, default=None, help="Output path for score.json")
    ap.add_argument("--out-critique", type=str, default=None, help="Output path for critique.md")
    ap.add_argument("--banned-words", type=str,
                    default=str(Path(".claude/skills/sdd-req100/banned_words.txt")),
                    help="Path to banned_words.txt")
    args = ap.parse_args()

    req_path = Path(args.requirements_md)
    if not req_path.exists():
        print(json.dumps({"error": f"File not found: {req_path}"}))
        return

    md = req_path.read_text(encoding="utf-8")

    # Load banned words
    banned_path = Path(args.banned_words)
    if not banned_path.exists():
        # fallback for user-level skill install
        banned_path = Path.home() / ".claude/skills/sdd-req100/banned_words.txt"
    banned_words = load_banned_words(banned_path) if banned_path.exists() else []

    # ---- Parse sections ----
    sections = [m.group(1).strip() for m in RE_SECTION.finditer(md)]
    required_sections = [
        "ç›®çš„", "æ¦‚è¦", "ã‚¹ã‚³ãƒ¼ãƒ—", "ç”¨èªé›†", "å‰æ/ä»®å®š", "å‰æ", "ä»®å®š",
        "æ©Ÿèƒ½è¦ä»¶", "éæ©Ÿèƒ½è¦ä»¶", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼",
        "ãƒ­ã‚°", "ç›£è¦–", "é‹ç”¨", "æœªè§£æ±ºäº‹é …"
    ]

    # Check which required sections are missing
    found_sections = set()
    for sec in sections:
        for req in required_sections:
            if req in sec:
                found_sections.add(req)

    # Core required sections
    core_required = ["ç›®çš„", "ã‚¹ã‚³ãƒ¼ãƒ—", "æ©Ÿèƒ½è¦ä»¶", "éæ©Ÿèƒ½è¦ä»¶", "æœªè§£æ±ºäº‹é …"]
    missing_sections = [s for s in core_required if s not in found_sections and
                        not any(s in sec for sec in sections)]

    # ---- Parse requirements blocks ----
    reqs = []
    matches = list(RE_REQ_HEADER.finditer(md))
    for i, m in enumerate(matches):
        start = m.start()
        end = matches[i+1].start() if i+1 < len(matches) else len(md)
        block = md[start:end]
        req_id = m.group(1)
        title = m.group(2)

        t = RE_TYPE_LINE.search(block)
        p = RE_PRIORITY_LINE.search(block)
        e = RE_EARS_LINE.search(block)
        ats = RE_AT_LINE.findall(block)
        ehs = RE_EH_LINE.findall(block)

        ears_line = e.group(1).strip() if e else ""
        reqs.append({
            "id": req_id,
            "title": title,
            "has_type": bool(t),
            "has_priority": bool(p),
            "has_ears": bool(e),
            "ears": ears_line,
            "ears_ok": ears_ok(ears_line) if ears_line else False,
            "ats": ats,
            "ehs": ehs,
            "has_at": len(ats) > 0,
            "has_eh": len(ehs) > 0,
            "gwt_ok": any(("Given" in a and "When" in a and "Then" in a) for a in ats),
        })

    # ---- Open Questions count ----
    open_q_count = 0
    oq_match = re.search(r"^##\s+.*æœªè§£æ±ºäº‹é ….*$(.*?)(^##\s+|\Z)", md, flags=re.MULTILINE | re.DOTALL)
    if oq_match:
        body = oq_match.group(1)
        # Count bullet points that aren't just "ãªã—" or empty
        bullets = re.findall(r"^\s*-\s+(.+)", body, flags=re.MULTILINE)
        for b in bullets:
            if b.strip() and b.strip() not in ["ãªã—", "ï¼ˆãªã—ï¼‰", "None", "N/A", "-"]:
                open_q_count += 1

    # ---- Scoring (C/U/T/E each 0..25) ----
    C = 25
    U = 25
    T = 25
    E = 25

    violations = []

    # Completeness
    C -= 3 * len(missing_sections)
    for s in missing_sections:
        violations.append({
            "code": "MISSING_SECTION",
            "severity": "high",
            "message": f"å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸è¶³: {s}"
        })

    # Per requirement mandatory fields
    for r in reqs:
        for field, ok in [
            ("ç¨®åˆ¥", r["has_type"]),
            ("å„ªå…ˆåº¦", r["has_priority"]),
            ("è¦ä»¶æ–‡(EARS)", r["has_ears"]),
            ("å—å…¥ãƒ†ã‚¹ãƒˆ", r["has_at"]),
            ("ä¾‹å¤–ãƒ»ã‚¨ãƒ©ãƒ¼", r["has_eh"])
        ]:
            if not ok:
                C -= 1
                violations.append({
                    "code": "MISSING_REQ_FIELD",
                    "severity": "high",
                    "message": f"{r['id']} ã«å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¬ è½: {field}"
                })

    # Open questions penalty
    if open_q_count > 0:
        C -= 2 * open_q_count
        violations.append({
            "code": "OPEN_QUESTIONS",
            "severity": "high",
            "message": f"æœªè§£æ±ºäº‹é …ãŒ {open_q_count} ä»¶ã‚ã‚Šã¾ã™ï¼ˆ100ç‚¹åˆ°é”ä¸å¯ï¼‰ã€‚"
        })

    C = max(0, min(25, C))

    # Unambiguity
    banned_total, banned_hits = count_banned(md, banned_words)
    U -= min(25, banned_total)
    if banned_total:
        violations.append({
            "code": "AMBIGUOUS_WORDS",
            "severity": "medium",
            "message": f"æ›–æ˜§èªãŒ {banned_total} å€‹å‡ºç¾: {banned_hits}"
        })
    U = max(0, min(25, U))

    # Testability
    for r in reqs:
        if not r["has_at"]:
            T -= 3
            violations.append({
                "code": "NO_ACCEPTANCE_TEST",
                "severity": "high",
                "message": f"{r['id']} ã«å—å…¥ãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“"
            })
        elif not r["gwt_ok"]:
            T -= 1
            violations.append({
                "code": "GWT_NOT_OK",
                "severity": "medium",
                "message": f"{r['id']} ã®å—å…¥ãƒ†ã‚¹ãƒˆãŒGWT(Given/When/Then)å½¢å¼ã«è¦‹ãˆã¾ã›ã‚“"
            })
    T = max(0, min(25, T))

    # EARS compliance
    for r in reqs:
        if r["has_ears"] and not r["ears_ok"]:
            E -= 2
            violations.append({
                "code": "EARS_NOT_OK",
                "severity": "high",
                "message": f"{r['id']} ã®è¦ä»¶æ–‡(EARS)ãŒEARSãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã—ã¾ã›ã‚“: {r['ears']}"
            })
        if not r["has_ears"]:
            E -= 2
    E = max(0, min(25, E))

    score_total = int(C + U + T + E)

    score = {
        "score_total": score_total,
        "score_breakdown": {"C": C, "U": U, "T": T, "E": E},
        "counts": {
            "requirements": len(reqs),
            "missing_sections": len(missing_sections),
            "open_questions": open_q_count,
            "banned_word_total": banned_total,
        },
        "violations": violations,
    }

    # Generate critique
    critique_lines = []
    critique_lines.append("# Critique\n\n")
    critique_lines.append(f"- **Score**: {score_total}/100 (C={C}, U={U}, T={T}, E={E})\n")
    critique_lines.append(f"- **Requirements found**: {len(reqs)}\n")
    critique_lines.append(f"- **Missing sections**: {len(missing_sections)}\n")
    critique_lines.append(f"- **Open questions**: {open_q_count}\n")
    critique_lines.append(f"- **Banned words**: {banned_total}\n\n")

    if len(reqs) == 0:
        critique_lines.append("## Critical\n")
        critique_lines.append("- è¦ä»¶(REQ-xxx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ãƒ³ãƒ—ãƒ¬å½¢å¼ã«æ²¿ã£ã¦ãã ã•ã„ã€‚\n\n")

    if violations:
        critique_lines.append("## Issues\n\n")
        for v in violations[:50]:  # Limit to 50 issues
            severity_icon = "ğŸ”´" if v['severity'] == 'high' else "ğŸŸ¡"
            critique_lines.append(f"- {severity_icon} [{v['severity']}] **{v['code']}**: {v['message']}\n")
    else:
        critique_lines.append("## Issues\n\n")
        critique_lines.append("- é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n")

    critique_lines.append("\n## Recommendations\n\n")
    if score_total >= 98:
        critique_lines.append("- ã‚¹ã‚³ã‚¢ã¯ç›®æ¨™(98ç‚¹ä»¥ä¸Š)ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\n")
    else:
        critique_lines.append(f"- ç›®æ¨™ã‚¹ã‚³ã‚¢(98ç‚¹)ã¾ã§ã‚ã¨ {98 - score_total} ç‚¹å¿…è¦ã§ã™ã€‚\n")
        if C < 25:
            critique_lines.append("- **Completeness**: å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³/ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚\n")
        if U < 25:
            critique_lines.append("- **Unambiguity**: æ›–æ˜§èªã‚’å…·ä½“çš„ãªæ•°å€¤/çŠ¶æ…‹ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚\n")
        if T < 25:
            critique_lines.append("- **Testability**: å…¨REQã«GWTå½¢å¼ã®å—å…¥ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚\n")
        if E < 25:
            critique_lines.append("- **EARS**: å…¨REQã®è¦ä»¶æ–‡ã‚’EARSãƒ‘ã‚¿ãƒ¼ãƒ³ã«æº–æ‹ ã•ã›ã¦ãã ã•ã„ã€‚\n")

    # Write outputs
    if args.out_json:
        Path(args.out_json).parent.mkdir(parents=True, exist_ok=True)
        Path(args.out_json).write_text(json.dumps(score, ensure_ascii=False, indent=2), encoding="utf-8")

    if args.out_critique:
        Path(args.out_critique).parent.mkdir(parents=True, exist_ok=True)
        Path(args.out_critique).write_text("".join(critique_lines), encoding="utf-8")

    # Print summary for terminal
    print(json.dumps(score, ensure_ascii=False))


if __name__ == "__main__":
    main()
